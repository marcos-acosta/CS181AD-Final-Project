{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "dc3409e0",
   "metadata": {},
   "source": [
    "## 2. Apply word2vec to Wilding chord dataset\n",
    "We wish to apply the skip-gram word2vec model to learn embeddings from each chord. Ideally, chords that appear in similar contexts will have similar word vectors.\n",
    "\n",
    "We will follow [this Tensorflow tutorial](https://www.tensorflow.org/tutorials/text/word2vec) to train to model with negative sampling."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "449587b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import tqdm\n",
    "import io\n",
    "\n",
    "from pathlib import Path\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers\n",
    "from tensorflow.keras.preprocessing.sequence import skipgrams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f1763883",
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext tensorboard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a07ee3e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "AUTOTUNE = tf.data.AUTOTUNE\n",
    "SEED = 42"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "3af43da7",
   "metadata": {},
   "outputs": [],
   "source": [
    "CHORDS_SAVEPATH = Path('chords.txt')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56adb7b0",
   "metadata": {},
   "source": [
    "As a reminder, the skip-gram model works by asking a neural net to maximize the probability of predicting \"nearby\" words or \"neighbor words\" given a single word.\n",
    "\n",
    "<img src=\"https://miro.medium.com/max/1200/0*FTfdlZ7yDBoQ8c9W.png\" width=\"500px\">\n",
    "\n",
    "**Negative sampling** is an optimization technique. Without it, the model would need to update the weights of *every* word that is unrelated to the target word (everywhere it should output 0). This scales with the vocabulary, so negative sampling offers a way of only updating a few of these \"negative\" words to save time and boost performance. The original paper recommends using 5-20 words for a smaller dataset, so we'll choose 20 negative samples."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "e95e59ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generates skip-gram pairs with negative sampling for a list of sequences\n",
    "# (int-encoded sentences) based on window size, number of negative samples\n",
    "# and vocabulary size.\n",
    "def generate_training_data(sequences, window_size, num_ns, vocab_size, seed):\n",
    "    # Elements of each training example are appended to these lists.\n",
    "    targets, contexts, labels = [], [], []\n",
    "\n",
    "    # Build the sampling table for `vocab_size` tokens.\n",
    "    sampling_table = tf.keras.preprocessing.sequence.make_sampling_table(vocab_size)\n",
    "\n",
    "    # Iterate over all sequences (sentences) in the dataset.\n",
    "    for sequence in tqdm.tqdm(sequences):\n",
    "        # Generate positive skip-gram pairs for a sequence (sentence).\n",
    "        positive_skip_grams, _ = tf.keras.preprocessing.sequence.skipgrams(\n",
    "              sequence,\n",
    "              vocabulary_size=vocab_size,\n",
    "              sampling_table=sampling_table,\n",
    "              window_size=window_size,\n",
    "              negative_samples=0)\n",
    "\n",
    "        # Iterate over each positive skip-gram pair to produce training examples\n",
    "        # with a positive context word and negative samples.\n",
    "        for target_word, context_word in positive_skip_grams:\n",
    "            context_class = tf.expand_dims(\n",
    "                tf.constant([context_word], dtype=\"int64\"), 1)\n",
    "            negative_sampling_candidates, _, _ = tf.random.log_uniform_candidate_sampler(\n",
    "                true_classes=context_class,\n",
    "                num_true=1,\n",
    "                num_sampled=num_ns,\n",
    "                unique=True,\n",
    "                range_max=vocab_size,\n",
    "                seed=SEED,\n",
    "                name=\"negative_sampling\")\n",
    "\n",
    "            # Build context and label vectors (for one target word)\n",
    "            negative_sampling_candidates = tf.expand_dims(\n",
    "                negative_sampling_candidates, 1)\n",
    "\n",
    "            context = tf.concat([context_class, negative_sampling_candidates], 0)\n",
    "            label = tf.constant([1] + [0]*num_ns, dtype=\"int64\")\n",
    "\n",
    "            # Append each element from the training example to global lists.\n",
    "            targets.append(target_word)\n",
    "            contexts.append(context)\n",
    "            labels.append(label)\n",
    "\n",
    "    return targets, contexts, labels"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39a6f88a",
   "metadata": {},
   "source": [
    "Create a TF TextLineDataset from the chords we parsed earlier."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "486a9455",
   "metadata": {},
   "outputs": [],
   "source": [
    "text_ds = tf.data.TextLineDataset(CHORDS_SAVEPATH).filter(lambda x: tf.cast(tf.strings.length(x), bool))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "847773a6",
   "metadata": {},
   "source": [
    "Find the maximul length sequence:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "73541217",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "93"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "max_len = 0\n",
    "for x in text_ds:\n",
    "    length = len(x.numpy().decode(\"utf-8\").split())\n",
    "    max_len = max(max_len, length)\n",
    "max_len"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0a1d738",
   "metadata": {},
   "source": [
    "### Define vectorizer\n",
    "\n",
    "Now we must define a vectorization layer for our Word2Vec model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "df1ba8aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the vocabulary size and the number of words in a sequence.\n",
    "vocab_size = 4096\n",
    "sequence_length = max_len\n",
    "\n",
    "# Use the `TextVectorization` layer to normalize, split, and map strings to\n",
    "# integers. Set the `output_sequence_length` length to pad all samples to the\n",
    "# same length.\n",
    "vectorize_layer = layers.TextVectorization(\n",
    "    standardize=lambda data: data,\n",
    "    max_tokens=vocab_size,\n",
    "    pad_to_max_tokens=True,\n",
    "    output_mode='int')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "90653765",
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorize_layer.adapt(text_ds.batch(1024))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87b22e55",
   "metadata": {},
   "source": [
    "Let's print the first 20 tokens in our vocabulary:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "757d62cc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['', '[UNK]', 'G7', 'Dm7', 'Am7', 'CM7', 'C', 'C7', 'A7', 'E7', 'D7', 'F7', 'Em7', 'FM7', 'B%7', 'Am', 'Eaug7', 'Bb7', 'Gm7', 'B7']\n"
     ]
    }
   ],
   "source": [
    "inverse_vocab = vectorize_layer.get_vocabulary()\n",
    "print(inverse_vocab[:20])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "e16f9a02",
   "metadata": {},
   "outputs": [],
   "source": [
    "text_vector_ds = text_ds.batch(1024).prefetch(AUTOTUNE).map(vectorize_layer).unbatch()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "211b8a2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "sequences = list(text_vector_ds.as_numpy_iterator())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0663217",
   "metadata": {},
   "source": [
    "And, we can illustrate our vectorizer in action:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "bae65728",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[15  3  9 15  8  3  2  5 13 14 19 89  9 45  4 16  9 66 15 69  4 16  9 66\n",
      " 15 15 30 19  9  8  3  2  7 11 19  9 15 30 19  9  8  3  2  7 11  9 15  8\n",
      " 31  8 31  8 31  8 66  8 31 14  9  8 10 18  7 11 17 11 17 11  9 15  0  0\n",
      "  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0] => ['Am', 'Dm7', 'E7', 'Am', 'A7', 'Dm7', 'G7', 'CM7', 'FM7', 'B%7', 'B7', 'EM', 'E7', 'AM7', 'Am7', 'Eaug7', 'E7', 'Do7', 'Am', 'Am,M7', 'Am7', 'Eaug7', 'E7', 'Do7', 'Am', 'Am', 'Gb%7', 'B7', 'E7', 'A7', 'Dm7', 'G7', 'C7', 'F7', 'B7', 'E7', 'Am', 'Gb%7', 'B7', 'E7', 'A7', 'Dm7', 'G7', 'C7', 'F7', 'E7', 'Am', 'A7', 'Dm', 'A7', 'Dm', 'A7', 'Dm', 'A7', 'Do7', 'A7', 'Dm', 'B%7', 'E7', 'A7', 'D7', 'Gm7', 'C7', 'F7', 'Bb7', 'F7', 'Bb7', 'F7', 'E7', 'Am', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '']\n",
      "[ 5 37 11 39 34 22 29  3  2  5  4  3  2  5 37 11 39 34 22 29  3  2  5  4\n",
      "  3  2  5  4  3  2 52 42  3  2  5 37 11 39 34 22 29  3  2  5  0  0  0  0\n",
      "  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0\n",
      "  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0] => ['CM7', 'Cm7', 'F7', 'BbM7', 'Bbm7', 'Eb7', 'AbM7', 'Dm7', 'G7', 'CM7', 'Am7', 'Dm7', 'G7', 'CM7', 'Cm7', 'F7', 'BbM7', 'Bbm7', 'Eb7', 'AbM7', 'Dm7', 'G7', 'CM7', 'Am7', 'Dm7', 'G7', 'CM7', 'Am7', 'Dm7', 'G7', 'Dbm7', 'Gb7', 'Dm7', 'G7', 'CM7', 'Cm7', 'F7', 'BbM7', 'Bbm7', 'Eb7', 'AbM7', 'Dm7', 'G7', 'CM7', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '']\n"
     ]
    }
   ],
   "source": [
    "for seq in sequences[:2]:\n",
    "  print(f\"{seq} => {[inverse_vocab[i] for i in seq]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76596cf0",
   "metadata": {},
   "source": [
    "### Prepare data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "cd95cd9c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████████████████████████| 76/76 [00:00<00:00, 1958.25it/s]\n"
     ]
    }
   ],
   "source": [
    "targets, contexts, labels = generate_training_data(\n",
    "    sequences=sequences,\n",
    "    window_size=2,\n",
    "    num_ns=20,\n",
    "    vocab_size=vocab_size,\n",
    "    seed=SEED)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "aa0dc522",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "targets.shape: (227,)\n",
      "contexts.shape: (227, 21)\n",
      "labels.shape: (227, 21)\n"
     ]
    }
   ],
   "source": [
    "targets = np.array(targets)\n",
    "contexts = np.array(contexts)[:,:,0]\n",
    "labels = np.array(labels)\n",
    "print(f\"targets.shape: {targets.shape}\")\n",
    "print(f\"contexts.shape: {contexts.shape}\")\n",
    "print(f\"labels.shape: {labels.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4e499ff",
   "metadata": {},
   "source": [
    "Now we create a dataset from our targets, contexts, and labels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "e2eb4609",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<BatchDataset element_spec=((TensorSpec(shape=(16,), dtype=tf.int64, name=None), TensorSpec(shape=(16, 21), dtype=tf.int64, name=None)), TensorSpec(shape=(16, 21), dtype=tf.int64, name=None))>\n"
     ]
    }
   ],
   "source": [
    "BATCH_SIZE = 16\n",
    "BUFFER_SIZE = 50\n",
    "dataset = tf.data.Dataset.from_tensor_slices(((targets, contexts), labels))\n",
    "dataset = dataset.shuffle(BUFFER_SIZE).batch(BATCH_SIZE, drop_remainder=True)\n",
    "print(dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "45818c7a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<PrefetchDataset element_spec=((TensorSpec(shape=(16,), dtype=tf.int64, name=None), TensorSpec(shape=(16, 21), dtype=tf.int64, name=None)), TensorSpec(shape=(16, 21), dtype=tf.int64, name=None))>\n"
     ]
    }
   ],
   "source": [
    "dataset = dataset.cache().prefetch(buffer_size=AUTOTUNE)\n",
    "print(dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e84839d3",
   "metadata": {},
   "source": [
    "### Define and train model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "219d8c4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Word2Vec(tf.keras.Model):\n",
    "    def __init__(self, vocab_size, embedding_dim, num_ns):\n",
    "        super(Word2Vec, self).__init__()\n",
    "        self.target_embedding = layers.Embedding(vocab_size,\n",
    "                                                 embedding_dim,\n",
    "                                                 input_length=1,\n",
    "                                                 name=\"w2v_embedding\")\n",
    "        self.context_embedding = layers.Embedding(vocab_size,\n",
    "                                                  embedding_dim,\n",
    "                                                  input_length=num_ns+1)\n",
    "\n",
    "    def call(self, pair):\n",
    "        target, context = pair\n",
    "        # target: (batch, dummy?)\n",
    "        # context: (batch, context)\n",
    "        if len(target.shape) == 2:\n",
    "            target = tf.squeeze(target, axis=1)\n",
    "        # target: (batch,)\n",
    "        word_emb = self.target_embedding(target)\n",
    "        # word_emb: (batch, embed)\n",
    "        context_emb = self.context_embedding(context)\n",
    "        # context_emb: (batch, context, embed)\n",
    "        dots = tf.einsum('be,bce->bc', word_emb, context_emb)\n",
    "        # dots: (batch, context)\n",
    "        return dots"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9686d3b9",
   "metadata": {},
   "source": [
    "We'll more or less arbitrarily define our embedding dimension to be 128, and choose 4 negative samples per positive sample."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "id": "cf96e1b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding_dim = 128\n",
    "word2vec = Word2Vec(vocab_size, embedding_dim, num_ns=4)\n",
    "word2vec.compile(optimizer='adam',\n",
    "                 loss=tf.keras.losses.CategoricalCrossentropy(from_logits=True),\n",
    "                 metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "id": "33595c27",
   "metadata": {},
   "outputs": [],
   "source": [
    "tensorboard_callback = tf.keras.callbacks.TensorBoard(log_dir=\"logs\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8be2dc5a",
   "metadata": {},
   "source": [
    "Now we fit our Word2Vec model to our dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "id": "544d1fd1",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/30\n",
      "14/14 [==============================] - 0s 6ms/step - loss: 3.0436 - accuracy: 0.0714\n",
      "Epoch 2/30\n",
      "14/14 [==============================] - 0s 8ms/step - loss: 3.0225 - accuracy: 0.4732\n",
      "Epoch 3/30\n",
      "14/14 [==============================] - 0s 8ms/step - loss: 3.0007 - accuracy: 0.6116\n",
      "Epoch 4/30\n",
      "14/14 [==============================] - 0s 9ms/step - loss: 2.9725 - accuracy: 0.6473\n",
      "Epoch 5/30\n",
      "14/14 [==============================] - 0s 9ms/step - loss: 2.9338 - accuracy: 0.6696\n",
      "Epoch 6/30\n",
      "14/14 [==============================] - 0s 9ms/step - loss: 2.8815 - accuracy: 0.6741\n",
      "Epoch 7/30\n",
      "14/14 [==============================] - 0s 10ms/step - loss: 2.8131 - accuracy: 0.6562\n",
      "Epoch 8/30\n",
      "14/14 [==============================] - 0s 8ms/step - loss: 2.7276 - accuracy: 0.6652\n",
      "Epoch 9/30\n",
      "14/14 [==============================] - 0s 10ms/step - loss: 2.6258 - accuracy: 0.6607\n",
      "Epoch 10/30\n",
      "14/14 [==============================] - 0s 9ms/step - loss: 2.5108 - accuracy: 0.6473\n",
      "Epoch 11/30\n",
      "14/14 [==============================] - 0s 11ms/step - loss: 2.3874 - accuracy: 0.6384\n",
      "Epoch 12/30\n",
      "14/14 [==============================] - 0s 10ms/step - loss: 2.2611 - accuracy: 0.6339\n",
      "Epoch 13/30\n",
      "14/14 [==============================] - 0s 10ms/step - loss: 2.1368 - accuracy: 0.6429\n",
      "Epoch 14/30\n",
      "14/14 [==============================] - 0s 9ms/step - loss: 2.0178 - accuracy: 0.6429\n",
      "Epoch 15/30\n",
      "14/14 [==============================] - 0s 11ms/step - loss: 1.9058 - accuracy: 0.6562\n",
      "Epoch 16/30\n",
      "14/14 [==============================] - 0s 10ms/step - loss: 1.8012 - accuracy: 0.6652\n",
      "Epoch 17/30\n",
      "14/14 [==============================] - 0s 9ms/step - loss: 1.7040 - accuracy: 0.6607\n",
      "Epoch 18/30\n",
      "14/14 [==============================] - 0s 11ms/step - loss: 1.6138 - accuracy: 0.6652\n",
      "Epoch 19/30\n",
      "14/14 [==============================] - 0s 10ms/step - loss: 1.5303 - accuracy: 0.6696\n",
      "Epoch 20/30\n",
      "14/14 [==============================] - 0s 11ms/step - loss: 1.4531 - accuracy: 0.6920\n",
      "Epoch 21/30\n",
      "14/14 [==============================] - 0s 11ms/step - loss: 1.3820 - accuracy: 0.7054\n",
      "Epoch 22/30\n",
      "14/14 [==============================] - 0s 10ms/step - loss: 1.3166 - accuracy: 0.7366\n",
      "Epoch 23/30\n",
      "14/14 [==============================] - 0s 10ms/step - loss: 1.2567 - accuracy: 0.7366\n",
      "Epoch 24/30\n",
      "14/14 [==============================] - 0s 11ms/step - loss: 1.2019 - accuracy: 0.7455\n",
      "Epoch 25/30\n",
      "14/14 [==============================] - 0s 11ms/step - loss: 1.1520 - accuracy: 0.7545\n",
      "Epoch 26/30\n",
      "14/14 [==============================] - 0s 12ms/step - loss: 1.1065 - accuracy: 0.7545\n",
      "Epoch 27/30\n",
      "14/14 [==============================] - 0s 11ms/step - loss: 1.0651 - accuracy: 0.7500\n",
      "Epoch 28/30\n",
      "14/14 [==============================] - 0s 9ms/step - loss: 1.0276 - accuracy: 0.7545\n",
      "Epoch 29/30\n",
      "14/14 [==============================] - 0s 12ms/step - loss: 0.9934 - accuracy: 0.7589\n",
      "Epoch 30/30\n",
      "14/14 [==============================] - 0s 11ms/step - loss: 0.9624 - accuracy: 0.7634\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7fd9c57206d8>"
      ]
     },
     "execution_count": 169,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word2vec.fit(dataset, epochs=30, callbacks=[tensorboard_callback])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "15b15e22",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Reusing TensorBoard on port 6006 (pid 72687), started 0:27:01 ago. (Use '!kill 72687' to kill it.)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "      <iframe id=\"tensorboard-frame-615bce8c3e19f4a8\" width=\"100%\" height=\"800\" frameborder=\"0\">\n",
       "      </iframe>\n",
       "      <script>\n",
       "        (function() {\n",
       "          const frame = document.getElementById(\"tensorboard-frame-615bce8c3e19f4a8\");\n",
       "          const url = new URL(\"/\", window.location);\n",
       "          const port = 6006;\n",
       "          if (port) {\n",
       "            url.port = port;\n",
       "          }\n",
       "          frame.src = url;\n",
       "        })();\n",
       "      </script>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%tensorboard --logdir logs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77da0b3a",
   "metadata": {},
   "source": [
    "### Save embedding matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae300745",
   "metadata": {},
   "source": [
    "Let's load the weights and vocabulary from the embedding layer of our Word2Vec model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "id": "a0e93ca5",
   "metadata": {},
   "outputs": [],
   "source": [
    "weights = word2vec.get_layer('w2v_embedding').get_weights()[0]\n",
    "vocab = vectorize_layer.get_vocabulary()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99b8bbe6",
   "metadata": {},
   "source": [
    "Finally, let's save the embeddings and vocabulary to file (`.tsv` for tab-separated-values)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "id": "b9b65c4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "Path('embeddings/wilding-w2v/').mkdir(exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "id": "ed208fea",
   "metadata": {},
   "outputs": [],
   "source": [
    "out_v = io.open('embeddings/wilding-w2v/embedding.tsv', 'w', encoding='utf-8')\n",
    "out_m = io.open('embeddings/wilding-w2v/vocab.tsv', 'w', encoding='utf-8')\n",
    "\n",
    "for index, word in enumerate(vocab):\n",
    "    if index == 0:\n",
    "        continue  # skip 0, it's padding.\n",
    "    vec = weights[index]\n",
    "    out_v.write('\\t'.join([str(x) for x in vec]) + \"\\n\")\n",
    "    out_m.write(word + \"\\n\")\n",
    "out_v.close()\n",
    "out_m.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2896d61c",
   "metadata": {},
   "source": [
    "These embeddings can be loaded in the [Tensorflow embedding projector](https://projector.tensorflow.org/), as shown below.\n",
    "\n",
    "<img src=\"images/wilding_embeddings.png\" width=\"500px\">"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "863f2f5c",
   "metadata": {},
   "source": [
    "### Explore chord similarity\n",
    "\n",
    "Let's experimentally verify whether the chord embeddings have learned something about the similarity of chords."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "id": "6bbb7757",
   "metadata": {},
   "outputs": [],
   "source": [
    "def cosine_similarity(a, b):\n",
    "    return np.dot(a, b) / (np.linalg.norm(a) * np.linalg.norm(b))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "id": "6802a33f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Our \"starting\" chord\n",
    "chord = 'D7'\n",
    "# Gbo7 is another voicing for D7b9\n",
    "# Ab7 is what's known as the tritone substitution for D7\n",
    "# Gb%7 is another voicing for D9\n",
    "substitutes = ['Gbo7', 'Ab7', 'Gb%7']\n",
    "# These chords are very unrelated to D7\n",
    "not_substitutes = ['EM7', 'FM7', 'Dbm7']\n",
    "# These are chords that often follow D7\n",
    "next_chords = ['Gm7', 'G7', 'GM7']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "id": "fd9d0145",
   "metadata": {},
   "outputs": [],
   "source": [
    "chord_vec = vectorize_layer([chord])[0, 0].numpy()\n",
    "subs_vec = np.squeeze(vectorize_layer(substitutes).numpy(), axis=-1)\n",
    "not_subs_vec = np.squeeze(vectorize_layer(not_substitutes).numpy(), axis=-1)\n",
    "next_chords_vec = np.squeeze(vectorize_layer(next_chords).numpy(), axis=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "id": "04af701f",
   "metadata": {},
   "outputs": [],
   "source": [
    "chord_embedding = weights[chord_vec]\n",
    "subs_embeddings = [weights[i] for i in subs_vec]\n",
    "not_subs_embeddings = [weights[i] for i in not_subs_vec]\n",
    "next_chords_embeddings = [weights[i] for i in next_chords_vec]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "id": "785e5e4e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SUBSTITUTES\n",
      "D7 and Gbo7: 0.11925406754016876\n",
      "D7 and Ab7: 0.03695443272590637\n",
      "D7 and Gb%7: -0.07435572147369385\n"
     ]
    }
   ],
   "source": [
    "print(\"SUBSTITUTES\")\n",
    "for i, similar in enumerate(subs_embeddings):\n",
    "    similarity = cosine_similarity(chord_embedding, subs_embeddings[i])\n",
    "    print(f\"{chord} and {substitutes[i]}: {similarity}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "id": "9e555315",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NOT SUBSTITUTES\n",
      "D7 and EM7: -0.10294283926486969\n",
      "D7 and FM7: -0.02963419258594513\n",
      "D7 and Dbm7: 0.11339980363845825\n"
     ]
    }
   ],
   "source": [
    "print(\"NOT SUBSTITUTES\")\n",
    "for i, similar in enumerate(not_subs_embeddings):\n",
    "    similarity = cosine_similarity(chord_embedding, not_subs_embeddings[i])\n",
    "    print(f\"{chord} and {not_substitutes[i]}: {similarity}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "id": "2476a908",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NEXT CHORDS\n",
      "D7 and Gm7: -0.19272486865520477\n",
      "D7 and G7: -0.09428456425666809\n",
      "D7 and GM7: 0.0013562807580456138\n"
     ]
    }
   ],
   "source": [
    "print(\"NEXT CHORDS\")\n",
    "for i, similar in enumerate(next_chords_embeddings):\n",
    "    similarity = cosine_similarity(chord_embedding, next_chords_embeddings[i])\n",
    "    print(f\"{chord} and {next_chords[i]}: {similarity}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab5eec24",
   "metadata": {},
   "source": [
    "Overall, this is not very promising. It's difficult to glean from these similarities whether the Word2Vec model is actually learning anything about the function of chords in a jazz context. Next, we'll try to fit an LSTM model to the chords in this dataset and see how it performs."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
