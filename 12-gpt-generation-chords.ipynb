{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "18498372",
   "metadata": {},
   "source": [
    "## Generating chords with GPT2\n",
    "\n",
    "We now use the model trained in `11-gpt-model-chords.ipynb` to generate chords."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8c75944d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import GPT2LMHeadModel, GPT2TokenizerFast, GPT2Config\n",
    "\n",
    "import torch\n",
    "from torch.utils.data import Dataset\n",
    "from tqdm import tqdm\n",
    "from pathlib import Path\n",
    "import numpy as np\n",
    "import os\n",
    "import re\n",
    "from music21 import stream, chord, duration"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4adb1466",
   "metadata": {},
   "source": [
    "### Load and prepare model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "aaa4b4ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "TOKENIZER_SAVEDIR = Path('tokenizers/chord-augmented-tokenizer/')\n",
    "LM_MODEL_SAVEDIR = Path('models/gpt-chords-augmented/')\n",
    "TXT_FILES = Path('chords-txt-augmented/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e33fd651",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = GPT2TokenizerFast.from_pretrained(TOKENIZER_SAVEDIR, \n",
    "                                              bos_token=\"<start>\", \n",
    "                                              eos_token=\"</start>\",\n",
    "                                              unk_token=\"<unk>\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "1c13c2a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "pad_token_id, eos_token_id = tokenizer.encode('<pad> </start>')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d05c914a",
   "metadata": {},
   "outputs": [],
   "source": [
    "config = GPT2Config(\n",
    "    vocab_size=tokenizer.vocab_size,\n",
    "    n_head=12,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "6456ce10",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = GPT2LMHeadModel(config=config).from_pretrained(str(LM_MODEL_SAVEDIR))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71a66f75",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "a089595a",
   "metadata": {},
   "source": [
    "### Define helper functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "8d4ee3c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "notes = ['C', 'D-', 'D', 'E-', 'E', 'F', 'G-', 'G', 'A-', 'A', 'B-', 'B']\n",
    "\n",
    "def abridged_index_to_note(abridged_index):\n",
    "    return notes[abridged_index % 12] + str(abridged_index // 12 + 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "aeba9021",
   "metadata": {},
   "outputs": [],
   "source": [
    "def token_to_notes(token):\n",
    "    arr = np.array([int(x) for x in token])\n",
    "    ones = np.where(arr == 1)[0]\n",
    "    return ' '.join([abridged_index_to_note(index) for index in ones])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "4eb3b6c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def note_to_abdridged_index(note):\n",
    "    note, octave = note[:-1], int(note[-1])\n",
    "    return (octave - 3) * 12 + notes.index(note)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "a085c05d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def notes_to_token(notes):\n",
    "    notes = notes.split()\n",
    "    indexes = [note_to_abdridged_index(note) for note in notes]\n",
    "    arr = np.zeros(36, dtype=np.uint8)\n",
    "    for i in indexes:\n",
    "        arr[i] = 1\n",
    "    return ''.join([str(x) for x in arr])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9cab80dd",
   "metadata": {},
   "source": [
    "Sanity check conversion of token to notes and notes to token:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "1313e77f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'B-3 D-4 E4 B-4'"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "token_to_notes('000000000010010010000010000000000000')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "5a1e9fa0",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "000010001010000100000000000000000000\n",
      "000010001000000100000000000000000000\n",
      "000010000010000100000000000000000000\n"
     ]
    }
   ],
   "source": [
    "print(notes_to_token('E3 A-3 B-3 E-4'))\n",
    "print(notes_to_token('E3 A-3 E-4'))\n",
    "print(notes_to_token('E3 B-3 E-4'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01ef7429",
   "metadata": {},
   "source": [
    "### Generate example outputs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f49f9072",
   "metadata": {},
   "source": [
    "We'll make a seed consisting of the chord progression `C6 A7b9 Dm7`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "b88fc228",
   "metadata": {},
   "outputs": [],
   "source": [
    "seed_tokens = [\n",
    "    '<start>', \n",
    "    notes_to_token('G3 A3 C4 E4'),\n",
    "    notes_to_token('G3 B-3 D-4 E4'),\n",
    "    notes_to_token('A3 C4 D4 F4'),\n",
    "]\n",
    "seed = ' '.join(seed_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "f377bea7",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_str = tokenizer.encode(seed, return_tensors=\"pt\")\n",
    "output_tokens = model.generate(input_str, \n",
    "                               pad_token_id=pad_token_id,\n",
    "                               eos_token_id=eos_token_id,\n",
    "                               temperature=1,\n",
    "                               max_length=256,\n",
    "                               do_sample=True,\n",
    "                               num_beams=5)[0]\n",
    "output_tokens = tokenizer.decode(output_tokens).split()\n",
    "output_tokens = [t for t in output_tokens if t[0] != '<']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "4146d6c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def arr_to_midi(arr, savepath):\n",
    "    ''' Convert a chord array to MIDI that can be opened in a DAW '''\n",
    "    score = stream.Score()\n",
    "    for i, timestep in enumerate(arr):\n",
    "        notes = np.where(timestep == 1)[0]\n",
    "        notes_str = [pitch_index_to_pitch_str(idx + 27) for idx in notes]\n",
    "        chord_ = chord.Chord(notes_str)\n",
    "        chord_.duration = duration.Duration('quarter')\n",
    "        chord_.offset = i\n",
    "        score.append(chord_)\n",
    "    score.write('midi', savepath)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "faef2f85",
   "metadata": {},
   "outputs": [],
   "source": [
    "def output_to_midi(tokens, savepath):\n",
    "    ''' Convert output from GPT to MIDI '''\n",
    "    tokens = [t for t in tokens if t[0] != '<']\n",
    "    tokens = [np.array([int(digit) for digit in t]) for t in tokens]\n",
    "    token_arr = np.array(tokens)\n",
    "    full_arr = np.zeros((len(token_arr), 88))\n",
    "    full_arr[:, 27:63] = token_arr\n",
    "    arr_to_midi(full_arr, savepath)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "d123643e",
   "metadata": {},
   "outputs": [],
   "source": [
    "index_to_step = ['C', 'D-', 'D', 'E-', 'E', 'F', 'G-', 'G', 'A-', 'A', 'B-', 'B', ]\n",
    "\n",
    "def pitch_index_to_pitch_str(pitch_index):\n",
    "    pitch_index += 9\n",
    "    return index_to_step[pitch_index % 12] + str(pitch_index // 12)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "id": "3913972e",
   "metadata": {},
   "outputs": [],
   "source": [
    "output_to_midi(output_tokens, 'test.mid')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf86f906",
   "metadata": {},
   "source": [
    "### Save embeddings to file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "85287508",
   "metadata": {},
   "outputs": [],
   "source": [
    "Path('embeddings/gpt-chords/').mkdir(exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "e9bbc0d3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████████████████████████████████████████████████████████| 6382/6382 [00:10<00:00, 605.80it/s]\n"
     ]
    }
   ],
   "source": [
    "embeddings = model.transformer.wte.weight\n",
    "with open('embeddings/gpt-chords/embedding.tsv', 'w') as f:\n",
    "    for row in tqdm(embeddings):\n",
    "        f.write('\\t'.join([str(col.item()) for col in row]) + \"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "88e9048d",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('embeddings/gpt-chords/vocab.tsv', 'w') as f:\n",
    "    for i in range(len(embeddings)):\n",
    "        f.write(tokenizer.decode([i]) + '\\n')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
