{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4d7eeddd",
   "metadata": {},
   "source": [
    "## 7. GPT Tokenizer\n",
    "\n",
    "Train a word-level tokenizer for GPT. The tokenizer simply splits on whitespace and indexes the `vocab_size` most common tokens."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ac834e47",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tokenizers import Tokenizer, Regex\n",
    "from tokenizers.models import WordLevel\n",
    "from tokenizers.trainers import WordLevelTrainer\n",
    "from tokenizers.pre_tokenizers import WhitespaceSplit, Split\n",
    "from pathlib import Path\n",
    "import os"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f847433",
   "metadata": {},
   "source": [
    "Define where to load text files and save tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "77294491",
   "metadata": {},
   "outputs": [],
   "source": [
    "TXT_LOCATION = Path('chords-txt-augmented/')\n",
    "TOKENIZER_SAVEDIR = Path('tokenizers/chord-augmented-tokenizer')\n",
    "TOKENIZER_SAVEDIR.mkdir(exist_ok=True, parents=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9d679f48",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = Tokenizer(WordLevel(unk_token=\"<unk>\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a26277ff",
   "metadata": {},
   "source": [
    "Define special tokens and vocabulary size. For the raw text and note pair GPT models, I use a large vocab size of about `30000`. For the cleaned chord dataset, I intentionally restrict the vocabulary size to `8000` because of how it is preprocessed (see `09-make-cleaned-chord-dataset.ipynb`)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e22dad6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "SPECIAL_TOKENS = [\n",
    "    \"<start>\",\n",
    "    \"</start>\",\n",
    "    \"<pad>\",\n",
    "    \"<unk>\",\n",
    "]\n",
    "VOCAB_SIZE = 8000\n",
    "trainer = WordLevelTrainer(show_progress=True, special_tokens=SPECIAL_TOKENS, vocab_size=VOCAB_SIZE)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75b21d30",
   "metadata": {},
   "source": [
    "### Create and train word-level tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68b8237f",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer.pre_tokenizer = WhitespaceSplit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "a8cf2614",
   "metadata": {},
   "outputs": [],
   "source": [
    "files = [str(TXT_LOCATION / path) for path in os.listdir(TXT_LOCATION)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "040998d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer.train(files, trainer)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ad98568",
   "metadata": {},
   "source": [
    "Let's do a sanity check to make sure the tokenizer works as expected:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "a8d9c43d",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(files[11], \"r\") as f:\n",
    "    text = f.read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "87b0eb2e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "000000000000001010010000001010000000 000010000010000100000000000000000000 000000000000000100010100001000000001 000000000000001000010000000000010000 000001000000000010000100000000000000 000010000000001000010001000000000000 000010000100100010010100000000000000 000000100000100010000000000010001001 000000010100100001000000000000000000 000001001001000010000000000000000000 000001000110001000010000000000000000 000000001000010000100010000000000000 000000100001000010001000000000000000 000010000100001000010000000000000000 100000000100100000000000000000000000 000000001000001000010000100010010000 000000000000000010000000100010000000 000000010000001010010000000100000000 000000000000001010000000001010000000 000000010000000001010000000000000000 000000010010010010000000000000000000 000000100000000000000000101000000000 000000010000000000100100000000000000 000001000001000100010000000000000000 000000000100001010000000000000000000 000010000010000100000000000000000000 000000000010000010000100000000000000 000000000100001100010000000000000000 000000000100000000000000001000000001 000000000100001100010000000000010000 000000000000100010000100000000000000 000000010000001000010001000000000000 000000000100100000000100000000000000 000000010000010001010000000000000000 000000100000101000000000000000000000 000000000100100001000100000010000000 000001000001000010001001000000000000 000000000010101000000000000000000000 000000010000010000100010000000000000 000000010000100010000100000000000000 000000000100001010010000000000000000 000000000001100010010000100000000000 000000001000001000010000000000010000 000000001000001000010000100010000000 000000010000001010000000001000000000 000000000000110000000000100000000000 000000000010001001000100100000000000 000000100000100010000100100000000000 000000000100001010010000001000000000 000010000000000010000100000000000000\n"
     ]
    }
   ],
   "source": [
    "text_sample = ' '.join(text.split()[:50])\n",
    "print(text_sample)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "36f08e6b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2970, 189, 4363, 892, 1870, 885, 4908, 1044, 186, 322]\n"
     ]
    }
   ],
   "source": [
    "# Encode the sample\n",
    "encoding = tokenizer.encode(text_sample)\n",
    "print(encoding.ids[:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "53b65449",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Decode the sample to get the original\n",
    "decoded = tokenizer.decode(encoding.ids)\n",
    "decoded == text_sample"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66ab353e",
   "metadata": {},
   "source": [
    "### Save tokenizer to file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "adfd53e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer.save(str(TOKENIZER_SAVEDIR / 'tokenizer.json'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "0c273869",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = Tokenizer.from_file(str(TOKENIZER_SAVEDIR / 'tokenizer.json'))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
