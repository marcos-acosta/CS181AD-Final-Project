{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5b958ba1",
   "metadata": {},
   "source": [
    "## 11. GPT for learning chords\n",
    "\n",
    "Now we apply Huggingface's GPT2 model to attempt to learn the chord representation using the cleaned chords we generated in `08-make-cleaned-chord-dataset.ipynb`. Each token is a 36-long string with `0`s and `1`s representing the active notes."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3447bf9",
   "metadata": {},
   "source": [
    "### Load necessary libraries and objects"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "baa7e978",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import GPT2Config, GPT2LMHeadModel, GPT2TokenizerFast, DataCollatorForLanguageModeling\n",
    "from transformers import Trainer, TrainingArguments\n",
    "\n",
    "import torch\n",
    "from torch.utils.data import Dataset\n",
    "from tqdm import tqdm\n",
    "from pathlib import Path\n",
    "import wandb\n",
    "import os\n",
    "import json\n",
    "import itertools\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "80fd1292",
   "metadata": {},
   "outputs": [],
   "source": [
    "TOKENIZER_SAVEDIR = Path('tokenizers/chord-augmented-tokenizer')\n",
    "LM_MODEL_SAVEDIR = Path('models/gpt-chords-augmented/')\n",
    "LM_MODEL_SAVEDIR.mkdir(exist_ok=True, parents=True)\n",
    "TXT_LOCATION = Path('chords-txt-augmented/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "680e567f",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.cuda.set_device(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "baf98283",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = GPT2TokenizerFast.from_pretrained(TOKENIZER_SAVEDIR, \n",
    "                                              bos_token=\"<start>\", \n",
    "                                              eos_token=\"</start>\",\n",
    "                                              unk_token=\"<unk>\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a092ee86",
   "metadata": {},
   "source": [
    "Sanity check tokenizer:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "77209fe1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('<start>', 0),\n",
       " ('</start>', 1),\n",
       " ('<pad>', 2),\n",
       " ('<unk>', 3),\n",
       " ('000000010010010010000000000000000000', 4),\n",
       " ('000000000010010010000000000000000000', 5),\n",
       " ('000000010000010010000000000000000000', 6),\n",
       " ('000000010000000000010000000000010000', 7),\n",
       " ('000010010010010010000000000000000000', 8),\n",
       " ('000000010010010010010000000000000000', 9)]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sorted(tokenizer.vocab.items(), key=lambda x: x[1])[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "1897dddc",
   "metadata": {},
   "outputs": [],
   "source": [
    "config = GPT2Config(\n",
    "    vocab_size=tokenizer.vocab_size,\n",
    "    n_head=12,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "bd10a425",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Num parameters: 90743808\n"
     ]
    }
   ],
   "source": [
    "model = GPT2LMHeadModel(config=config)\n",
    "print('Num parameters:', model.num_parameters())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0b18c61",
   "metadata": {},
   "source": [
    "### Define dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "5668d1dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomDataset(Dataset):\n",
    "    def __init__(self, src_files, tokenizer, max_len):\n",
    "        self.examples = []\n",
    "        self.pad_token = tokenizer.encode('<pad>')[0]\n",
    "        for src_file in tqdm(src_files):\n",
    "            words = src_file.read_text(encoding=\"utf-8\")\n",
    "            words = '<start> ' + words + ' </start>'\n",
    "            tokenized = tokenizer.encode(words)\n",
    "            for i in range(0, len(tokenized), max_len):\n",
    "                chunk = tokenized[i:i + max_len]\n",
    "                tensor = torch.ones(max_len, dtype=torch.int64) * self.pad_token\n",
    "                tensor[:len(chunk)] = torch.tensor(chunk)\n",
    "                self.examples.append(tensor)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.examples)\n",
    "\n",
    "    def __getitem__(self, i):\n",
    "        return self.examples[i]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "eae018e4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████████████████████████████████████████████████████████| 3408/3408 [00:09<00:00, 365.04it/s]\n"
     ]
    }
   ],
   "source": [
    "src_files = list(Path(TXT_LOCATION).glob(\"**/*.txt\"))\n",
    "dataset = CustomDataset(src_files, tokenizer, 128)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "109b8f05",
   "metadata": {},
   "source": [
    "Sanity check an example from the dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "48d1ce23",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'000000000000010010000010010000000000 000010000000000010000000000000010000 000000010000010000010100000000000000 010000000010000000000000010000000000 000000000000000010000010010000010000 000000000000000010010000000010010000 010000010010000000000000000000000000 000000000000010000010010000000000000 010000010010010010000000000000000000 000000000000000010000000000010010000 000010000000000000000000000010000010 000000000000010000010010010000000000 000000010000010000010000000000000000 010000010000010000010000000000000000 000000000010010010000010000000000000 000010010000000000000000000010000000 000010000000000000000100000010000000 000000010000000010000010010000000000 000010010000000010000000000000000000 000000000000000000000010000100010010 000010000000010000000000000000010000 000000010000010000000000000010000000 000000000000010000010000010000000000 000000000000000010000010010000000000 010000010000010000010010000000000000 000000000000010000000100010010000000 000010000000000000010000000000010000 000000000010100000000000000010000000 010000000010000000000000010000000000 000000110000000010000000000000000000 000110000000000000010000000000000000 000000010010000000010010010000000000 </start> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad>'"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.decode(dataset.__getitem__(108))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1704cb8a",
   "metadata": {},
   "source": [
    "### Training\n",
    "\n",
    "Now, we simply create a data collator, define the training arguments, train, and save the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "8b9a889f",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_collator = DataCollatorForLanguageModeling(tokenizer=tokenizer, mlm=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "c549ab8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "training_args = TrainingArguments(\n",
    "    output_dir=LM_MODEL_SAVEDIR,\n",
    "    overwrite_output_dir=True,\n",
    "    num_train_epochs=10,\n",
    "    per_device_train_batch_size=32,\n",
    "    save_steps=10000,\n",
    "    logging_steps=2000,\n",
    "    save_total_limit=1,\n",
    "    prediction_loss_only=False,\n",
    ")\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    data_collator=data_collator,\n",
    "    train_dataset=dataset,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b16e3d8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "ret = trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22947c8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer.save_model(LM_MODEL_SAVEDIR)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
